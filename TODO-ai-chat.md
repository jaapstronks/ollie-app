# TODO: In-App AI Chat (BYOK â€” Bring Your Own Key)

Add an AI chat tab where users can ask questions about their puppy's data using their own LLM API key. No server needed, no AI costs for us.

## Concept

The app has all the puppy data locally. An AI chat lets users ask natural questions:
- "Wanneer moet ze weer plassen?"
- "Hoe lang slaapt ze gemiddeld overdag?"
- "Gaat het beter met de zindelijkheid?"
- "Vergelijk deze week met vorige week"

The LLM gets the puppy's data as context and answers based on actual patterns.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chat UI (SwiftUI)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ğŸ’¬ "Hoe lang was ze wakker?"     â”‚ â”‚
â”‚  â”‚ ğŸ¤– "Gemiddeld 47 min vandaag,   â”‚ â”‚
â”‚  â”‚     langste was 1u12 na lunch."  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â†“ user message               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Context Builder                   â”‚ â”‚
â”‚  â”‚ - Today's events (JSONL)         â”‚ â”‚
â”‚  â”‚ - Calculated stats               â”‚ â”‚
â”‚  â”‚ - Puppy profile                  â”‚ â”‚
â”‚  â”‚ â†’ System prompt + user message   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚              â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ LLM Provider (user's API key)    â”‚ â”‚
â”‚  â”‚ OpenAI / Anthropic / Ollama      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Step 1: API Key Settings

Create `Views/Settings/AISettingsView.swift`:

```swift
struct AISettings: Codable {
    var provider: AIProvider
    var apiKey: String
    var model: String  // e.g. "gpt-4o-mini", "claude-sonnet-4-20250514"
    
    enum AIProvider: String, Codable, CaseIterable {
        case openai = "OpenAI"
        case anthropic = "Anthropic"
        case ollama = "Ollama (lokaal)"
    }
}
```

- Store API key in iOS Keychain (NOT UserDefaults â€” it's a secret)
- Provider picker: OpenAI, Anthropic, Ollama
- Model selector per provider (hardcoded sensible defaults)
- "Test verbinding" button
- For Ollama: custom base URL field (default: `http://localhost:11434`)

## Step 2: Context Builder

Create `Services/AIContextBuilder.swift`:

The context builder prepares the system prompt with relevant puppy data. Key principle: **send only what's needed**, not the entire history. Token budget matters.

```swift
class AIContextBuilder {
    /// Build system prompt with relevant context for a user question
    func buildContext(question: String, profile: PuppyProfile) -> String {
        var context = """
        Je bent een slimme puppy-assistent. Je helpt de eigenaar van \(profile.name), 
        een \(profile.breed ?? "puppy") geboren op \(profile.birthDate.formatted()).
        
        Antwoord in het Nederlands. Wees concreet en gebruik de data.
        Als je iets niet weet uit de data, zeg dat eerlijk.
        
        """
        
        // Always include: today's events
        context += "## Vandaag\n" + todayEventsAsText()
        
        // Always include: key stats
        context += "\n## Statistieken\n" + currentStatsAsText()
        
        // Conditionally include based on question topic:
        // - Sleep question â†’ include sleep breakdown
        // - Potty question â†’ include gap analysis + predictions
        // - Trend question â†’ include weekly comparison
        
        return context
    }
}
```

**Smart context selection** (keeps token usage low):
- Always: today's events + puppy age + key stats (potty %, streak)
- If question mentions sleep/dutje/nacht: add sleep analysis
- If question mentions plas/poep/zindelijk: add gap stats + predictions
- If question mentions week/trend/vergelijk: add weekly breakdown
- Fallback: include everything if question is vague

## Step 3: LLM Service

Create `Services/LLMService.swift`:

```swift
protocol LLMProvider {
    func chat(messages: [ChatMessage], model: String, apiKey: String) async throws -> String
}

class OpenAIProvider: LLMProvider { /* POST /v1/chat/completions */ }
class AnthropicProvider: LLMProvider { /* POST /v1/messages */ }
class OllamaProvider: LLMProvider { /* POST /api/chat */ }
```

- Streaming support (show response as it arrives)
- Error handling: invalid key, rate limit, network error
- Token counting (optional, for cost awareness)
- Timeout handling

## Step 4: Chat UI

Create `Views/Chat/ChatView.swift`:

- Tab bar item: ğŸ’¬ Chat
- Message list (LazyVStack, scroll to bottom)
- Text input with send button
- Typing indicator while waiting for response
- Messages persist in local storage (per conversation, max ~50 messages)
- "Nieuw gesprek" button to clear context
- First-time: prompt to set up API key in settings

Design: keep it simple, native iOS feel. No custom bubbles â€” use alternating background colors like iMessage.

## Step 5: Suggested Questions

Show suggestion chips when chat is empty:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stel een vraag over [puppy]... â”‚
â”‚                                  â”‚
â”‚  ğŸ’¡ Suggesties:                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Wanneer moet ze plassen?  â”‚  â”‚
â”‚  â”‚ Hoe ging het vandaag?     â”‚  â”‚
â”‚  â”‚ Slaapt ze genoeg?         â”‚  â”‚
â”‚  â”‚ Tips voor zindelijkheid   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Step 6: Tool Use (optional, advanced)

Instead of pre-building context, let the LLM request specific data via tool/function calls:

```json
{
    "name": "get_events",
    "parameters": { "date": "2026-02-19", "type": "plassen" }
}
{
    "name": "get_stats",
    "parameters": { "metric": "potty_gaps", "days": 7 }
}
{
    "name": "get_predictions",
    "parameters": {}
}
```

This is more flexible and token-efficient but requires OpenAI/Anthropic function calling support. Ollama support varies. Consider this a v2 of the chat feature.

## Privacy & Cost Considerations

- **API key stays on device** (Keychain) â€” never sent to our servers
- **Data stays on device** â€” sent directly to user's chosen LLM provider
- **Cost is user's responsibility** â€” show estimated token usage per message
- **Ollama option** â€” fully offline, zero cost, full privacy
- **No account needed** â€” the AI feature works with just an API key

## Recommended Default Models

| Provider | Model | Why |
|----------|-------|-----|
| OpenAI | gpt-4o-mini | Cheap, fast, good enough for data Q&A |
| Anthropic | claude-sonnet-4-20250514 | Best reasoning, great with structured data |
| Ollama | llama3.2 | Free, local, decent quality |

## Done Criteria
- [ ] Can configure API key for OpenAI, Anthropic, or Ollama
- [ ] Chat tab with message history
- [ ] AI responses use actual puppy data as context
- [ ] Streaming responses (text appears as it generates)
- [ ] Suggested questions on empty chat
- [ ] API key stored securely in Keychain
- [ ] Works offline with Ollama
- [ ] Clear error messages for auth/network issues

Delete this file when done.
